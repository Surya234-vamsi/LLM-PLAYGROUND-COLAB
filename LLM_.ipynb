{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkjuhCtTGnjs"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TrWVzZd8hmz",
    "outputId": "a126dc4c-4850-4668-a29c-2552ea499e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Ask your questions.\n",
      "\n",
      "Ask Anything (or exit): what is self attention mechanism in transformers architecture model?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Self attention mechanism in transformers architecture model is a type of attention mechanism where the model learns to attend to different parts of the input sequence and pay attention to each part independently by learning a weighting distribution for each token. This weighting distribution is then used to adjust the relative importance of each token when computing the output of the model, resulting in a more focused and selective attention to different parts of the input sequence.\n",
      "\n",
      "Python Code Example:\n",
      "\n",
      "```python\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, hidden_size, num_heads \n",
      "\n",
      "Ask Anything (or exit): exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=120,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "def ask_ai(q):\n",
    "    prompt = (\n",
    "        \"Explain the following clearly in 4–5 sentences. \"\n",
    "        \"Do NOT continue text patterns like Q&A, stories, chapters, books, or examples.\\n\\n\"\n",
    "        f\"{q}\\n\\n\"\n",
    "        \"Explanation:\"\n",
    "    )\n",
    "\n",
    "    output = pipe(prompt)[0][\"generated_text\"]\n",
    "    final = output.split(\"Explanation:\", 1)[-1].strip()\n",
    "    return final\n",
    "\n",
    "print(\"Model loaded. Ask your questions.\\n\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"Ask Anything (or exit): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    answer = ask_ai(q)\n",
    "    print(\"\\nAI:\", answer, \"\\n\")\n",
    "\n",
    "''' Since, \"microsoft/phi-1_5\" is a not a chat model it is not suitable for QA and chat type output. so, it is drifting very much.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1SPOexpG1Au"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487,
     "referenced_widgets": [
      "3e09d28d590d460d9e57440ed7d05d8a",
      "057a90bb767f4f23b4b7ee842824fea0",
      "d6517e044f9b4935b3915fcc21c75bb6",
      "bcd06e8c84d449d8b2cfb531185a87c6",
      "181da27d439448c4b5374a7340609dd1",
      "aad48fe3ab2c4fc8879329df803db0c2",
      "218fdedddbb64818a02ea2b88000f4d3",
      "a429fa3c611a44ab80579f0671910276",
      "272f5405cb76412e83997e84fada2cb0",
      "71cc00f2ee1542f8bb0ace9b384e75bd",
      "a7c2fa6f11b045cd8516ef1fd6da0f34",
      "3bca166c103d4279966944ba8f09c990",
      "45e59a88dc7544189068db200a1e3b74",
      "5170d6a015174faa809411504f988d67",
      "404737c84d5b4e3e9a25d5359e9800c2",
      "16827d79b08e42b8a141d2935645e7b5",
      "b3b08cd4d91b4d2ba09b18f61fb2b7f6",
      "c442b975339a490bbdec7b5ef73c04cf",
      "26927679511f47849f553ebc9c051a61",
      "4975b208c6b240099e4116ad74cba00d",
      "1e2cd97e9acf4301afed7fa1daa97964",
      "3c25b9a4c1fc497982169f1694e8e294",
      "4ce0670c53f44da2aeb3012f1518375c",
      "405808995c96466b88392fc1f6eb00df",
      "7a985ee587c948feaa0f743fb2bd0877",
      "e380b85787b54c3e905ae007e9b47ead",
      "01afff3c87164bfc9d8a04c8409095ca",
      "10b83bf8a8864ad39fd381221cc70a2c",
      "63616fdef8d14d5db0554bb2ca6ac862",
      "bf7b149d42154e8db82718a7bba881df",
      "63d7bbfd6ad746278e4b1bc6e6bfb3b0",
      "134a684dddb3426ab319db7d58f8a540",
      "d55ee1ff7a844c6fae7d91a1d5e11c9e",
      "9c06b12cd7624875bed3c026dae6672c",
      "9dd7181d9eb045c096bc8e83b10f33eb",
      "42e3b3bece7d47b9a5bb471c7e7acfc9",
      "1b31d4496f184d01bc6d8ad38ccec9ff",
      "78482bbbda904b189906b77c83becd91",
      "5bbaa9b749bb465082072a8defb99983",
      "57b9198290264071b6acca4c32991022",
      "273f205def854c9d81249da83469e6a4",
      "4490c5468a35497dab058f56d512cc60",
      "7b432a2ca55147668e82c679c7a52ec3",
      "7500cf69ea4a40aba9c8078421e78bae",
      "cd6bec9dce0c416d930182e550eeaef8",
      "df3054a2b72b42d6af721400f0199c5a",
      "025402358d3745db948b9c0ab50b6486",
      "b9520cc468c849db813235aac76a9aaf",
      "2c6c9e98474e4df1b02fe501daead35e",
      "4b954d5ef132470cac38ddafb6d54bd0",
      "78b2da6281fc44dfbef6bd0659252378",
      "cee81bc10c414d2db9c34a35b2fa0d4b",
      "b3f33b1bd90a4ec6819336760ff31c78",
      "be842c00e1ab423698edf49aad6ba9d7",
      "0cb91ff8f48546d590690d694b7fa0d1",
      "7fa3657e7dab4c18b7c616237a8f085e",
      "3ce6bb705efe4c489a5f2091a0482f68",
      "df4a6a4531334ae99075a58ee5154ed5",
      "bf1a2c01699b4f1f8a17ea8b28a534bd",
      "7216b29f609748f194ccbeabe82236ed",
      "6b6e2659cb194ee8996bbca21fbf9cbe",
      "7f0401292b85432b8ab14fd72ea39429",
      "af2296076c784cff8655e993fed1e725",
      "f90299ad0e4d4c409a8e9823ca6d7075",
      "0ae4c3b4d291479ca35a0f695d58327c",
      "0e737c877e154575a95256264742f5ae",
      "06d296f82cb144bfaa23284a8229b65b",
      "262e18229e8d4260964504ec8803f589",
      "e32ce67eaee94a28abc0271664c39109",
      "ff74f5b15e6d4bc8b8602cb710d4d440",
      "6256463bd9dd44fdaaa0b71927d64b83",
      "8ab8946c41dd4a808a3e0bd28b212d01",
      "8c8d995f7aa742ba91dfbae6421cd0e2",
      "333e19acbf4540828bb8c7ebe5b17c55",
      "f181892215b844a4ab69bb89563912aa",
      "5d765cae91424c8396e0538e35509911",
      "e48681d44f9d4ac89bb53d22bca94b14"
     ]
    },
    "id": "h_r-THO4G09v",
    "outputId": "497f2b26-9846-4c16-a655-218f4e92990f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e09d28d590d460d9e57440ed7d05d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bca166c103d4279966944ba8f09c990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce0670c53f44da2aeb3012f1518375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c06b12cd7624875bed3c026dae6672c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6bec9dce0c416d930182e550eeaef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa3657e7dab4c18b7c616237a8f085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d296f82cb144bfaa23284a8229b65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Ask your questions.\n",
      "\n",
      "Ask Anything (or exit): what is self attention mechanism in transformers architecture model?\n",
      "\n",
      "AI: Self-Attention is a key component of the Transformer architecture model, which is a type of neural network designed for natural language processing (NLP). In the Transformer architecture, each layer of the network is composed of multiple self-attention blocks, which are responsible for computing the attention weights between each input token and its corresponding attention head.\n",
      "\n",
      "The self-attention mechanism works by taking a sequence of input tokens as input and computing the attention weights between each token and its corresponding attention head. This attention weight is calculated as the product of the input token's position in the sequence and the cosine similarity between the input token's embedding and the query embedding.\n",
      "\n",
      "The attention weights are then used to compute the output of the layer, which is the output of the self-attention block. The output of the self-attention block is a vector of size d_model, where d_model is the dimension of the input token's embedding. The output \n",
      "\n",
      "Ask Anything (or exit): exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,\n",
    "    return_full_text=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def ask_ai(q):\n",
    "    # Chat template internally handles system/user roles\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    result = pipe(prompt)[0][\"generated_text\"]\n",
    "    return result.strip()\n",
    "\n",
    "print(\"Model loaded. Ask your questions.\\n\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"Ask Anything (or exit): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    answer = ask_ai(q)\n",
    "    print(\"\\nAI:\", answer, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0-1ZlHLPAFk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D25lJw0iY0r-",
    "outputId": "4f1e887a-f4a6-412b-ff71-a79286ffb8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.53.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.1.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.12.3)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (4.12.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2.32.4)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain_google_genai) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (0.4.56)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (0.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain_google_genai) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain_google_genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: filetype, langchain_google_genai\n",
      "Successfully installed filetype-1.2.0 langchain_google_genai-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dha2cv5zPACw",
    "outputId": "7ba04b96-798f-43bc-e60b-ecdc913eceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Ask your questions.\n",
      "\n",
      "Ask Anything (or exit): what is the use of self attention in transformers architecture?\n",
      "\n",
      "AI: response: Self-attention is the **cornerstone** of the Transformer architecture, and its primary use is to allow the model to **weigh the importance of different words (or tokens) in the input sequence relative to a given word** when processing that word.\n",
      "\n",
      "Here's a breakdown of its key uses and benefits:\n",
      "\n",
      "1.  **Contextual Understanding (The Primary Use):**\n",
      "    *   **Problem:** Words often have different meanings depending on their context. For example, \"bank\" can mean a financial institution or the side of a river. Traditional models (like simple RNNs) struggled to capture these dynamic contextual meanings because they processed words sequentially, largely based on immediate neighbors.\n",
      "    *   **Self-Attention's Solution:** When the model processes a word, say \"bank\" in the sentence \"I walked to the **bank** of the river,\" self-attention allows it to look at *all* other words in the sentence (\"I,\" \"walked,\" \"to,\" \"the,\" \"of,\" \"the,\" \"river\") and determine which ones are most relevant to understanding *this specific instance* of \"bank.\" It will assign higher \"attention scores\" to \"river\" and \"of the\" than to \"I\" or \"walked,\" thus correctly inferring the meaning.\n",
      "\n",
      "2.  **Handling Long-Range Dependencies:**\n",
      "    *   **Problem:** In long sentences or documents, related words can be far apart. RNNs struggled with this \"long-term dependency\" problem because information had to pass through many sequential steps, often decaying or getting diluted.\n",
      "    *   **Self-Attention's Solution:** It creates a direct connection between any two words in the sequence, regardless of their distance. When computing the representation for a word, it can directly \"attend\" to another word 50 tokens away just as easily as it attends to an adjacent word. This greatly improves the model's ability to understand complex relationships across long texts.\n",
      "\n",
      "3.  **Parallelization:**\n",
      "    *   **Problem:** Recurrent Neural Networks (RNNs) are inherently sequential. You must process word `n` before you can process word `n+1`. This makes training very slow on modern hardware (GPUs) which are designed for parallel computation.\n",
      "    *   **Self-Attention's Solution:** Since each word can attend to all other words simultaneously, the computations for all words in a sequence can be performed in parallel. This dramatically speeds up training and inference, making Transformers much more efficient than RNNs for long sequences.\n",
      "\n",
      "4.  **Dynamic Weighting (Feature Extraction):**\n",
      "    *   For each word, self-attention computes a weighted sum of all other words' representations in the sequence. These weights are dynamic, meaning they change based on the specific context of the input. This allows the model to create a rich, context-aware representation for each word.\n",
      "\n",
      "5.  **Global Information Integration:**\n",
      "    *   Every token in the output of a self-attention layer incorporates information from *every* other token in the input sequence. This means that each token's representation becomes a highly contextualized vector that encapsulates global information about the entire input.\n",
      "\n",
      "6.  **Interpretability (to some extent):**\n",
      "    *   The attention weights can sometimes provide a degree of interpretability. By visualizing which words attend to which other words, researchers can get insights into what the model is focusing on when making a prediction.\n",
      "\n",
      "**In essence, self-attention allows the Transformer to build a rich, context-aware representation for each word in a sequence by dynamically weighing the relevance of all other words.** This fundamentally changed how models process sequential data, leading to significant breakthroughs in NLP and beyond. \n",
      "\n",
      "Ask Anything (or exit): exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# GOOGLE_API_KEY = \"AIzaSyBUYyc9L-Ml1kzWT_np3Tl8PgZEBmTpB6Q\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# 1. Create the LLM object\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # fast, cheap, good for learning :contentReference[oaicite:1]{index=1}\n",
    "    temperature=0.5           # higher = more creative, lower = more deterministic\n",
    ")\n",
    "\n",
    "def ask_ai(q):\n",
    "    response = llm.invoke(q)\n",
    "    return f'response:\\n {response.content}'\n",
    "\n",
    "print(\"Model loaded. Ask your questions.\\n\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"Ask Anything (or exit): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    answer = ask_ai(q)\n",
    "    print(\"\\nAI:\", answer, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfNWeANAbvd9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nt7rqiScfjo",
    "outputId": "46fd326f-216a-47a7-eac7-34176fa06326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers\n",
      "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from ctransformers) (0.36.0)\n",
      "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ctransformers) (9.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (2025.11.12)\n",
      "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ctransformers\n",
      "Successfully installed ctransformers-0.2.27\n"
     ]
    }
   ],
   "source": [
    "! pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379,
     "referenced_widgets": [
      "543554ad2ddb4ac7b591a9e740db2e78",
      "0912744acf8e4f78b6b6ff03c6fbb34b",
      "180bb51295514fcf9bd1e92845237bf2",
      "9e817cc5b76c484e8067236619b53bca",
      "dd5a6a66af214b9cb47d4cb6120b8423",
      "5c09822ae66a479cb462725b5163d231",
      "1bd832054a624e5492d9ffdb794d894b",
      "d7bae3a2fddb44499c0ab4691e230659",
      "63e2cb1b392d4f6eb34518823d458432",
      "2c2d5e086a414c4f926b0cdbd4997b93",
      "0c986d5469f046cbb73914e3e7ca58c9",
      "6174aaeb5ef44d819b3cc6bbe8177491",
      "e02c910436f74f979cc66d40b648e291",
      "470c2e59e981457f85bafc54eb789a13",
      "c357f0b684ce49fb8e164ea9848746be",
      "e0b7e8a039b84212be9a2449e313c224",
      "f13607d107ca449aa6004ce87305d165",
      "d7591d72c7854062be2f5e08117bc02e",
      "f7b232322f1246a28772037bc5770389",
      "aab0bac80b724652922a18fba4de3f87",
      "321f2adf0f60440c813f7d5429951995",
      "91fac702c7a14ae78dc1d77180a48ebb"
     ]
    },
    "id": "r7OKndgClDku",
    "outputId": "522c5783-bb78-49ee-e866-8cea041b3e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543554ad2ddb4ac7b591a9e740db2e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6174aaeb5ef44d819b3cc6bbe8177491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "\n",
      "Ask Anything (or exit): what is the use of self attention in transformers architecture?\n",
      "\n",
      "AI: Self attention plays a crucial role in transformer architectures, as it allows the model to attend to specific parts of the input sequence and focus on them. This helps the model to generate better and more contextualized predictions, especially when dealing with long documents or multitask scenarios such as parsing large text corpora. Self attention can also help improve the model's performance by reducing the number of parameters required for each layer, making it easier to train and scale for large models on complex problems like machine translation. \n",
      "\n",
      "Ask Anything (or exit): what is the use of cross attention in transformers\n",
      "\n",
      "AI: Cross attention is a technique used in transformer models to improve the model's ability to understand and predict the contextual information that can be found in multiple sources, including those from different modalities. This helps the model to better integrate information across domains, making it more capable of anticipating user intent and generating relevant responses. \n",
      "\n",
      "Ask Anything (or exit): can you give me an image of transformers architecture\n",
      "\n",
      "AI: Sure, here's an image of Transformers architecture. https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Transformer_architecture.svg/1200px-Transformer_architecture.svg.png \n",
      "\n",
      "Ask Anything (or exit): exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    model_file=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "    context_length=512,\n",
    "    gpu_layers=0,\n",
    "    model_type=\"llama\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "def ask_ai(q):\n",
    "    prompt = f\"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
    "    output = llm(prompt)\n",
    "    return output.strip()\n",
    "\n",
    "while True:\n",
    "    q = input(\"Ask Anything (or exit): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    answer = ask_ai(q)\n",
    "    print(\"\\nAI:\", answer, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
