
# ğŸ“˜ Local LLM Playground â€” Colab Experiments (Transformers â€¢ ctransformers â€¢ Gemini API)

This repository contains a Google Colab notebook designed as a comprehensive playground for experimenting with different Large Language Models (LLMs) using:

- ğŸ¤– HuggingFace Transformers  
- âš¡ ctransformers (GGUF Local Inference)  
- ğŸ”¥ Google Gemini API  
- ğŸ§  Prompting & Chat Templates  
- ğŸ§ª Multiple Model Architectures (TinyLlama, Mistral, Phi, Qwen, etc.)

The notebook serves as a practical environment for learning, testing, comparing, and debugging LLM behavior in a single place.

---

## ğŸš€ Features

### âœ” Multiple LLM Backends
- HuggingFace Transformers (AutoModelForCausalLM + pipeline)
- ctransformers GGUF inference for running LLMs offline
- Google Gemini API for cloud-based high-quality generation

### âœ” Chatbot Implementations
- Custom Python chat loops  
- Multi-turn conversation handling  
- Consistent response formatting  
- Drift reduction techniques  

### âœ” Prompt Template Experiments
- LLaMA `[INST] ... [/INST]` format  
- Alpaca/Vicuna `### Instruction:` format  
- System/User role formatting  
- Gemini-style prompting  

### âœ” Model Comparison (Qualitative)
- TinyLlama vs Mistral vs Phi vs Qwen behavior  
- Drift, blank output, and hallucination analysis  
- How chat templates affect model performance  
- Local vs API inference differences  

---

## ğŸ“‚ Repository Contents

```
LLM-PLAYGROUND-COLAB/
â”‚
â”œâ”€â”€ LLM_.ipynb              # Main Colab notebook with all experiments
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ› ï¸ Requirements

Installable via Colab:

```bash
pip install transformers ctransformers google-generativeai langchain-google-genai huggingface_hub
```

---

## â–¶ï¸ How to Use

1. Open the notebook in Google Colab  
2. Install dependencies  
3. Choose a model backend:
   - HuggingFace Transformers  
   - GGUF model via ctransformers  
   - Gemini API  
4. Run chat functions to test responses  
5. Compare behavior between models  

---

## ğŸ” What You Will Learn

- How transformers and decoder-only models work  
- How to run LLMs offline with GGUF  
- Why some GGUF models produce blank or drifted answers  
- How to format prompts correctly for each architecture  
- Differences between TinyLlama, Mistral, Phi, and Qwen  
- How to integrate Gemini API into custom chatbots  

---

## âš ï¸ Known Limitations

- Some GGUF repos are removed or gated on HuggingFace  
- ctransformers does not support all architectures (e.g., Phi-2, Qwen2.x)  
- Mistral-based GGUF models may load but generate blank output  
- Chat templates must match model training format  
- Colab RAM may be insufficient for >4GB models  

These issues and solutions are documented inside the notebook.

---

## ğŸ¤ Contributions

Feel free to:

- Add new model experiments  
- Add benchmarks or evaluations  
- Improve instructions or templates  
- Add UI (Gradio) or API (FastAPI) extensions  

---

## ğŸ“œ License

This project is intended for learning and experimentation.  
Check individual model licenses before production use.

---
